# Technical Architecture & Design Document

## Project Overview

The VAPT Automation Pipeline is a modular, extensible system for automated vulnerability assessment and penetration testing. It integrates with Faraday for centralized vulnerability management and provides multi-channel alerting, ticket management, and professional compliance reporting.

## System Architecture

### High-Level Data Flow

```
Scan Tools Input
    ‚Üì
[Phase 1] Scan Automation & Auto-Import
    ‚îú‚îÄ scan_scheduler.py    (Nmap execution)
    ‚îú‚îÄ report_parser.py     (Multi-tool parsing)
    ‚îî‚îÄ Faraday API         (Auto-import)
    ‚Üì
[Phase 2] Severity-Based Alerting & Ticketing
    ‚îú‚îÄ alert_engine.py      (Multi-channel alerts)
    ‚îú‚îÄ ticket_manager.py    (Ticket lifecycle)
    ‚îî‚îÄ Faraday API         (Finding polling)
    ‚Üì
[Phase 3] DAST Scanning (Optional)
    ‚îî‚îÄ OWASP ZAP Integration (Coming soon)
    ‚Üì
[Phase 4] Compliance Reporting
    ‚îú‚îÄ report_generator.py  (HTML generation)
    ‚îú‚îÄ Jinja2 Templates    (Professional formatting)
    ‚îî‚îÄ Compliance Mapping  (OWASP/CIS)
    ‚Üì
Professional Reports & Intelligence
```

## Component Details

### Phase 1: Scan Automation & Auto-Import

#### `scan_scheduler.py`

**Purpose**: Execute Nmap scans and auto-import to Faraday

**Key Features**:
- Multiple scan profiles (quick, full, vuln)
- Timestamp-based result organization
- SHA256 deduplication hashing
- Structured logging
- Faraday API integration

**Workflow**:
```
Target Input
    ‚Üì
Validate Profile
    ‚Üì
Execute Nmap
    ‚Üì
Save XML (timestamped)
    ‚Üì
Parse XML locally
    ‚Üì
Load existing findings from Faraday
    ‚Üì
Deduplication check (CVE + Host + Port)
    ‚Üì
Import to Faraday REST API
    ‚Üì
Generate summary report
```

**Deduplication Logic**:
```python
hash_key = SHA256(f"{host}:{port}:{cve}")
if hash_key not in existing_findings:
    # New finding - import
else:
    # Duplicate - skip
```

#### `report_parser.py`

**Purpose**: Parse multi-tool scan reports

**Supported Formats**:
- Nessus (.nessus XML)
- OpenVAS (XML)
- Burp Suite (XML)

**Workflow**:
```
Report File
    ‚Üì
Auto-detect tool type
    ‚Üì
Parse tool-specific XML structure
    ‚Üì
Normalize severity (0-10 ‚Üí critical/high/medium/low/info)
    ‚Üì
Extract: host, port, CVE, CVSS, description
    ‚Üì
Push to Faraday API (/api/v3/workspaces/{ws}/vulns)
    ‚Üì
Return: pushed/failed counts
```

**Severity Normalization**:
- CVSS 9.0-10.0 ‚Üí Critical
- CVSS 7.0-8.9 ‚Üí High
- CVSS 5.0-6.9 ‚Üí Medium
- CVSS 3.0-4.9 ‚Üí Low
- CVSS 0-2.9 ‚Üí Info

### Phase 2: Severity-Based Alerting & Ticketing

#### `alert_engine.py`

**Purpose**: Monitor Faraday and send intelligent alerts

**Alert Channels**:
- **Slack**: Rich formatted messages with color coding
- **Email**: SMTP with HTML formatting
- **Webhooks**: Generic HTTP POST for external integrations

**Workflow**:
```
Poll Faraday API (configurable interval)
    ‚Üì
Get all findings since last check
    ‚Üì
Filter by severity threshold (configurable)
    ‚Üì
Check if finding already processed (memory + JSON file)
    ‚Üì
For each new finding:
    ‚îú‚îÄ Format alert message
    ‚îî‚îÄ Send to enabled channels
    ‚Üì
Save processed finding IDs
```

**Severity Filtering**:
```
Threshold: "high"
‚Üí Alert on: Critical, High
‚Üí Ignore: Medium, Low, Info

Threshold: "medium"
‚Üí Alert on: Critical, High, Medium
‚Üí Ignore: Low, Info
```

#### `ticket_manager.py`

**Purpose**: Manage ticket lifecycle for findings

**Ticket Status Flow**:
```
FINDING (from Faraday)
    ‚Üì
CREATE TICKET (severity ‚â• High)
    ‚îú‚îÄ Status: "open"
    ‚îú‚îÄ Priority: Based on CVSS
    ‚îî‚îÄ Created_at: timestamp
    ‚Üì
ACKNOWLEDGE (when triaged)
    ‚îú‚îÄ Status: "acknowledged"
    ‚îú‚îÄ Assigned_to: team member
    ‚îî‚îÄ Acknowledged_at: timestamp
    ‚Üì
RESOLVE (when fixed)
    ‚îú‚îÄ Status: "resolved"
    ‚îú‚îÄ Resolution_notes: What was done
    ‚îî‚îÄ Resolved_at: timestamp
```

**Priority Calculation**:
```python
if cvss_score >= 9.0:
    priority = "P0"  # Critical
elif cvss_score >= 7.0:
    priority = "P1"  # High
elif cvss_score >= 5.0:
    priority = "P2"  # Medium
else:
    priority = "P3"  # Low
```

**Ticket JSON Structure**:
```json
{
  "ticket_id": "TICKET-001",
  "vuln_name": "Vulnerability Name",
  "cve": "CVE-XXXX-XXXXX",
  "host": "192.168.1.100",
  "port": "443",
  "severity": "critical",
  "cvss": 9.8,
  "priority": "P0",
  "status": "open|acknowledged|resolved",
  "description": "...",
  "created_at": "2025-02-18T10:30:00",
  "acknowledged_at": null,
  "resolved_at": null,
  "resolution_notes": null,
  "assigned_to": null
}
```

### Phase 3: DAST Scanning

**Status**: Framework in place, integration pending

**Planned Features**:
- OWASP ZAP baseline scanning
- Auto-parse ZAP XML output
- Import web findings to separate workspace
- Differentiate DAST from network findings

### Phase 4: Compliance Reporting

#### `report_generator.py`

**Purpose**: Generate professional HTML compliance reports

**Workflow**:
```
Query Faraday API
    ‚Üì
Fetch all findings for workspace
    ‚Üì
Categorize by severity
    ‚Üì
Build compliance mapping:
    - Map findings to OWASP Top 10
    - Map findings to CIS Controls
    ‚Üì
Sort by CVSS score (for remediation priority)
    ‚Üì
Render Jinja2 template with data
    ‚Üì
Generate HTML report
```

**Report Sections**:
1. **Cover Page**: Title, workspace, date
2. **Executive Summary**: 
   - Finding counts by severity
   - Key findings highlighted
3. **Detailed Findings**: 
   - Critical/High: Detailed write-ups
   - Medium/Low: Table format
4. **Compliance Mapping**: 
   - Control-to-finding cross-reference
5. **Remediation Priority**: 
   - Top 20 by CVSS score
   - Action items
6. **Footer**: Generated timestamp

**Compliance Mapping Examples**:
```
Finding: "SQL Injection"
  ‚Üí OWASP: A03: Injection
  ‚Üí CIS: CIS 7: Vulnerability Management

Finding: "Weak Authentication"
  ‚Üí OWASP: A07: Identification and Authentication Failures
  ‚Üí CIS: CIS 6: Access Control Management
```

#### `templates/report_template.html`

**Features**:
- Professional CSS styling
- Color-coded severity levels
- Responsive design (mobile-friendly)
- Print-friendly formatting
- Jinja2 template variables
- Executive summary dashboard

### Master Orchestrator: `pipeline.py`

**Purpose**: Coordinate all phases

**Execution Flow**:
```
Parse CLI arguments
    ‚Üì
Validate environment (tools, credentials)
    ‚Üì
Phase 1: Execute scan_scheduler.py
    ‚Üì
Phase 2: Execute alert_engine.py + ticket_manager.py
    ‚Üì
Phase 3: Execute DAST (if enabled)
    ‚Üì
Phase 4: Execute report_generator.py
    ‚Üì
Generate execution summary
    ‚Üì
Display results and next steps
```

**Available Phases**:
- `--phases 1,2,4` (skip DAST)
- `--phases 1` (scan only)
- `--phases 2,4` (alerts and reporting)

## API Integrations

### Faraday REST API

**Endpoints Used**:

1. **Import Scan Results**
```
POST /api/v3/workspaces/{workspace}/import_scan
Content-Type: multipart/form-data

file: XML scan file
tool: "Nmap|Nessus|OpenVAS"
```

2. **Query Findings**
```
GET /api/v3/workspaces/{workspace}/vulns
Authorization: Bearer {API_KEY}

Returns: List of vulnerability objects
```

3. **Create/Update Findings**
```
POST /api/v3/workspaces/{workspace}/vulns
Authorization: Bearer {API_KEY}
Content-Type: application/json

{
  "name": "Vulnerability Name",
  "severity": "critical",
  "description": "...",
  "host": "192.168.1.100",
  "port": "443"
}
```

### External Integrations

**Slack Webhook**:
```
POST {webhook_url}
Content-Type: application/json

{
  "attachments": [{
    "color": "#FF0000",
    "title": "üö® Critical Vulnerability",
    "fields": [...]
  }]
}
```

**Email (SMTP)**:
```
Server: smtp.gmail.com:587
Auth: TLS
From: security@example.com
To: team@example.com
```

**Generic Webhooks**:
```
POST {webhook_url}
Content-Type: application/json

{
  "alert_type": "vulnerability",
  "severity": "critical",
  "host": "192.168.1.100",
  "vulnerability": "...",
  "timestamp": "2025-02-18T10:30:00Z"
}
```

## Configuration Management

### YAML Configuration (`config/settings.yaml`)

```yaml
faraday:
  url: "http://localhost:5985"
  api_key: "${FARADAY_API_KEY}"    # Via environment
  timeout: 30

scan_profiles:
  quick:    # -T4 -F (100 ports)
  full:     # -T4 -A -p- (all ports)
  vuln:     # --script vuln

alerts:
  channels:
    slack:
      enabled: true/false
      webhook_url: "${SLACK_WEBHOOK_URL}"
  severity_threshold: "high"

tickets:
  enabled: true
  severity_threshold: "high"
  priority_mapping:
    critical: "P0"
    high: "P1"
    # ...

reporting:
  compliance_framework: "owasp"    # or "cis"
```

### Environment Variables

```bash
FARADAY_API_KEY           # Required
SLACK_WEBHOOK_URL         # Optional
EMAIL_SMTP_SERVER         # Optional
EMAIL_FROM               # Optional
EMAIL_PASSWORD           # Optional
```

## Security Considerations

### Secrets Management

‚úÖ **DO**:
- Use environment variables for secrets
- Store GitHub Secrets securely
- Rotate API keys regularly
- Use HTTPS for external APIs

‚ùå **DON'T**:
- Commit API keys to repository
- Hardcode credentials in config files
- Share webhook URLs publicly
- Log sensitive data

### Scanner Ethics

‚ö†Ô∏è **CRITICAL**:
- Only scan systems you own or have written permission
- Never scan production systems without approval
- Document all scanning activities
- Respect network impact (use appropriate timeouts)

## Performance Optimization

### Large Network Scans

**Strategy**: Split into multiple ranges
```bash
python scan_scheduler.py --target 10.0.0.0/16 --profile quick
python scan_scheduler.py --target 10.1.0.0/16 --profile quick
# ... etc
```

### Parallel Execution

```bash
python scan_scheduler.py --target 192.168.1.0/24 --profile quick &
python scan_scheduler.py --target 10.0.0.0/24 --profile quick &
wait
```

### Database Optimization (Future)

- Implement finding cache with SQLite
- Index by CVE + Host + Port
- Historical trend tracking
- False positive ML classifier

## Extensibility

### Adding New Tools

1. **Create parser method**: `_parse_newtool()`
2. **Add to ReportParser class**
3. **Register in auto-detect logic**
4. **Update requirements.txt**

### Adding New Alert Channels

1. **Create `send_channel_alert()` method**
2. **Add configuration in settings.yaml**
3. **Update AlertEngine class**

### Adding Compliance Frameworks

1. **Extend mapping dictionaries**
2. **Add framework parameter**
3. **Create template variants**

## Future Enhancements

- [ ] CVSS v3 auto-enrichment from NVD API
- [ ] Trend dashboard with Flask
- [ ] ML-based false positive detection
- [ ] Jira/ServiceNow integration
- [ ] Multi-workspace aggregation
- [ ] Kubernetes deployment
- [ ] Container image scanning

## Troubleshooting Reference

| Issue | Root Cause | Solution |
|-------|-----------|----------|
| Faraday connection failed | API key invalid | Regenerate API key |
| Nmap permission denied | No sudo access | Setcap or run with sudo |
| Report not generated | No findings | Check workspace has findings |
| Slack alert not sent | Webhook invalid | Verify webhook URL |
| High memory usage | Large workspace | Process findings in batches |

---

**Version**: 1.0.0
**Last Updated**: 2025-02-18
**Maintainers**: Security Team
